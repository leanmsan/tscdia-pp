{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7511b2d4",
   "metadata": {},
   "source": [
    "# 03 - Procesamiento con PySpark\n",
    "\n",
    "Este notebook demuestra cómo usar PySpark para procesar datos de bases de datos a gran escala.\n",
    "\n",
    "## Objetivos:\n",
    "- Conectar a bases de datos usando Spark\n",
    "- Procesar datos de manera distribuida\n",
    "- Realizar transformaciones eficientes\n",
    "- Guardar resultados en diferentes formatos\n",
    "- Optimizar consultas para mejor rendimiento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b27f5038",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importar librerías necesarias\n",
    "import sys\n",
    "sys.path.append('../src')\n",
    "\n",
    "from etl.spark_loader import SparkDataLoader, create_spark_loader\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.types import *\n",
    "import yaml\n",
    "\n",
    "# Crear instancia de SparkDataLoader\n",
    "spark_loader = create_spark_loader(\"TSCDIA-Demo\")\n",
    "spark = spark_loader.spark\n",
    "\n",
    "print(\"✅ Spark inicializado correctamente\")\n",
    "print(f\"Versión de Spark: {spark.version}\")\n",
    "print(f\"Contexto: {spark.sparkContext.appName}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75c5e56e",
   "metadata": {},
   "source": [
    "## 1. Cargar Datos desde Base de Datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e03ed86e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configurar parámetros de conexión (ajustar según tu BD)\n",
    "db_params = {\n",
    "    \"url\": \"jdbc:postgresql://localhost:5432/mi_database\",\n",
    "    \"user\": \"mi_usuario\",\n",
    "    \"password\": \"mi_password\"\n",
    "}\n",
    "\n",
    "# Opción 1: Cargar tabla completa\n",
    "# df = spark_loader.read_from_postgres(\"mi_tabla\", **db_params)\n",
    "\n",
    "# Opción 2: Cargar con query personalizada\n",
    "# query = \"\"\"\n",
    "# SELECT \n",
    "#     id,\n",
    "#     fecha,\n",
    "#     categoria,\n",
    "#     valor,\n",
    "#     region\n",
    "# FROM mi_tabla \n",
    "# WHERE fecha >= '2024-01-01'\n",
    "# \"\"\"\n",
    "# df = spark_loader.read_from_postgres(query, **db_params)\n",
    "\n",
    "# Para este ejemplo, crear datos de muestra\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, DoubleType, DateType\n",
    "import datetime\n",
    "\n",
    "# Esquema de ejemplo\n",
    "schema = StructType([\n",
    "    StructField(\"id\", IntegerType(), True),\n",
    "    StructField(\"fecha\", DateType(), True),\n",
    "    StructField(\"categoria\", StringType(), True),\n",
    "    StructField(\"valor\", DoubleType(), True),\n",
    "    StructField(\"region\", StringType(), True)\n",
    "])\n",
    "\n",
    "# Datos de ejemplo\n",
    "data = [\n",
    "    (1, datetime.date(2024, 1, 1), \"A\", 100.0, \"Norte\"),\n",
    "    (2, datetime.date(2024, 1, 2), \"B\", 150.0, \"Sur\"),\n",
    "    (3, datetime.date(2024, 1, 3), \"A\", 200.0, \"Este\"),\n",
    "    (4, datetime.date(2024, 1, 4), \"C\", 175.0, \"Oeste\"),\n",
    "    (5, datetime.date(2024, 1, 5), \"B\", 120.0, \"Norte\")\n",
    "]\n",
    "\n",
    "df = spark.createDataFrame(data, schema)\n",
    "\n",
    "print(f\"Datos cargados: {df.count()} filas\")\n",
    "df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "369f57eb",
   "metadata": {},
   "source": [
    "## 2. Exploración Inicial con Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81dd07f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Información básica del DataFrame\n",
    "print(\"=== INFORMACIÓN DEL DATAFRAME ===\")\n",
    "print(f\"Número de filas: {df.count()}\")\n",
    "print(f\"Número de columnas: {len(df.columns)}\")\n",
    "print(f\"Columnas: {df.columns}\")\n",
    "\n",
    "# Schema\n",
    "print(\"\\n=== ESQUEMA ===\")\n",
    "df.printSchema()\n",
    "\n",
    "# Estadísticas descriptivas\n",
    "print(\"\\n=== ESTADÍSTICAS DESCRIPTIVAS ===\")\n",
    "df.describe().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1b5a756",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Análisis de calidad de datos\n",
    "quality_report = spark_loader.basic_data_quality_check(df)\n",
    "\n",
    "print(\"=== REPORTE DE CALIDAD ===\")\n",
    "print(f\"Total filas: {quality_report['total_rows']}\")\n",
    "print(f\"Total columnas: {quality_report['total_columns']}\")\n",
    "\n",
    "print(\"\\nAnálisis de valores nulos:\")\n",
    "for col, stats in quality_report['null_analysis'].items():\n",
    "    print(f\"  {col}: {stats['null_count']} nulos ({stats['null_percentage']:.2f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "526cf2f9",
   "metadata": {},
   "source": [
    "## 3. Transformaciones con Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec159f26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transformaciones básicas\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "# Agregar nuevas columnas\n",
    "df_transformed = df.withColumn(\"valor_log\", F.log(F.col(\"valor\"))) \\\n",
    "                   .withColumn(\"año\", F.year(F.col(\"fecha\"))) \\\n",
    "                   .withColumn(\"mes\", F.month(F.col(\"fecha\"))) \\\n",
    "                   .withColumn(\"valor_categoria\", \n",
    "                              F.when(F.col(\"valor\") > 150, \"Alto\")\n",
    "                               .when(F.col(\"valor\") > 100, \"Medio\")\n",
    "                               .otherwise(\"Bajo\"))\n",
    "\n",
    "print(\"DataFrame transformado:\")\n",
    "df_transformed.show()\n",
    "\n",
    "# Verificar schema actualizado\n",
    "print(\"\\nSchema actualizado:\")\n",
    "df_transformed.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e179ec6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Agregaciones y agrupaciones\n",
    "print(\"=== AGREGACIONES ===\")\n",
    "\n",
    "# Por categoría\n",
    "agg_categoria = df_transformed.groupBy(\"categoria\") \\\n",
    "    .agg(\n",
    "        F.count(\"*\").alias(\"total_registros\"),\n",
    "        F.sum(\"valor\").alias(\"valor_total\"),\n",
    "        F.avg(\"valor\").alias(\"valor_promedio\"),\n",
    "        F.min(\"valor\").alias(\"valor_minimo\"),\n",
    "        F.max(\"valor\").alias(\"valor_maximo\")\n",
    "    ).orderBy(F.desc(\"valor_total\"))\n",
    "\n",
    "print(\"Agregación por categoría:\")\n",
    "agg_categoria.show()\n",
    "\n",
    "# Por región\n",
    "agg_region = df_transformed.groupBy(\"region\") \\\n",
    "    .agg(\n",
    "        F.count(\"*\").alias(\"registros\"),\n",
    "        F.avg(\"valor\").alias(\"promedio\")\n",
    "    )\n",
    "\n",
    "print(\"Agregación por región:\")\n",
    "agg_region.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c772e66",
   "metadata": {},
   "source": [
    "## 4. Filtros y Consultas Complejas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0609dc35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filtros múltiples\n",
    "df_filtered = df_transformed.filter(\n",
    "    (F.col(\"valor\") > 120) & \n",
    "    (F.col(\"categoria\").isin([\"A\", \"B\"])) &\n",
    "    (F.col(\"region\") != \"Oeste\")\n",
    ")\n",
    "\n",
    "print(\"Datos filtrados:\")\n",
    "df_filtered.show()\n",
    "\n",
    "# Usar SQL sobre DataFrame\n",
    "df_transformed.createOrReplaceTempView(\"datos\")\n",
    "\n",
    "sql_result = spark.sql(\"\"\"\n",
    "    SELECT \n",
    "        categoria,\n",
    "        region,\n",
    "        COUNT(*) as total,\n",
    "        AVG(valor) as promedio,\n",
    "        PERCENTILE_APPROX(valor, 0.5) as mediana\n",
    "    FROM datos\n",
    "    WHERE valor > 100\n",
    "    GROUP BY categoria, region\n",
    "    ORDER BY promedio DESC\n",
    "\"\"\")\n",
    "\n",
    "print(\"\\nResultado de consulta SQL:\")\n",
    "sql_result.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30ceb3ab",
   "metadata": {},
   "source": [
    "## 5. Joins y Combinaciones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fb4e213",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crear DataFrame adicional para join\n",
    "schema_info = StructType([\n",
    "    StructField(\"categoria\", StringType(), True),\n",
    "    StructField(\"descripcion\", StringType(), True),\n",
    "    StructField(\"prioridad\", IntegerType(), True)\n",
    "])\n",
    "\n",
    "data_info = [\n",
    "    (\"A\", \"Categoría Premium\", 1),\n",
    "    (\"B\", \"Categoría Estándar\", 2),\n",
    "    (\"C\", \"Categoría Básica\", 3)\n",
    "]\n",
    "\n",
    "df_info = spark.createDataFrame(data_info, schema_info)\n",
    "\n",
    "print(\"DataFrame de información:\")\n",
    "df_info.show()\n",
    "\n",
    "# Realizar join\n",
    "df_joined = df_transformed.join(\n",
    "    df_info, \n",
    "    on=\"categoria\", \n",
    "    how=\"left\"\n",
    ")\n",
    "\n",
    "print(\"DataFrame con join:\")\n",
    "df_joined.select(\"id\", \"categoria\", \"descripcion\", \"valor\", \"prioridad\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3429569a",
   "metadata": {},
   "source": [
    "## 6. Optimización y Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f13090b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cache para DataFrames que se usan múltiples veces\n",
    "df_transformed.cache()\n",
    "\n",
    "# Verificar particiones\n",
    "print(f\"Número de particiones: {df_transformed.rdd.getNumPartitions()}\")\n",
    "\n",
    "# Reparticionar si es necesario\n",
    "df_repartitioned = df_transformed.repartition(2, \"categoria\")\n",
    "print(f\"Particiones después de repartition: {df_repartitioned.rdd.getNumPartitions()}\")\n",
    "\n",
    "# Explain plan para ver optimizaciones\n",
    "print(\"\\nPlan de ejecución:\")\n",
    "df_transformed.groupBy(\"categoria\").count().explain(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ec8432f",
   "metadata": {},
   "source": [
    "## 7. Guardar Resultados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60cddbae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Guardar como Parquet (formato recomendado)\n",
    "output_path = \"../data/processed/spark_output\"\n",
    "\n",
    "# Guardar particionado por categoría\n",
    "spark_loader.save_to_parquet(\n",
    "    df_transformed, \n",
    "    output_path + \"/parquet\", \n",
    "    partition_by=\"categoria\"\n",
    ")\n",
    "\n",
    "# Guardar como CSV (para compatibilidad)\n",
    "df_transformed.coalesce(1) \\\n",
    "    .write \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .option(\"header\", \"true\") \\\n",
    "    .csv(output_path + \"/csv\")\n",
    "\n",
    "# Guardar agregaciones\n",
    "agg_categoria.coalesce(1) \\\n",
    "    .write \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .option(\"header\", \"true\") \\\n",
    "    .csv(output_path + \"/agregaciones\")\n",
    "\n",
    "print(\"✅ Datos guardados exitosamente\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d8a0328",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Opcional: Escribir de vuelta a base de datos\n",
    "# spark_loader.write_to_database(\n",
    "#     df_transformed,\n",
    "#     \"tabla_procesada\",\n",
    "#     mode=\"overwrite\",\n",
    "#     **db_params\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49a1ff2b",
   "metadata": {},
   "source": [
    "## 8. Monitoreo y Métricas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29dbfb81",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtener métricas del job\n",
    "print(\"=== MÉTRICAS DE SPARK ===\")\n",
    "print(f\"Aplicación: {spark.sparkContext.appName}\")\n",
    "print(f\"ID de aplicación: {spark.sparkContext.applicationId}\")\n",
    "print(f\"Modo de deploy: {spark.sparkContext.deployMode}\")\n",
    "print(f\"Master: {spark.sparkContext.master}\")\n",
    "\n",
    "# Estadísticas de memoria\n",
    "status = spark.sparkContext.statusTracker()\n",
    "print(f\"\\nEjecutores activos: {len(status.getExecutorInfos())}\")\n",
    "\n",
    "# Acceder a Spark UI\n",
    "print(f\"\\nSpark UI disponible en: {spark.sparkContext.uiWebUrl}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04c8acd6",
   "metadata": {},
   "source": [
    "## 9. Limpieza y Cierre"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e146846",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Limpiar cache\n",
    "df_transformed.unpersist()\n",
    "\n",
    "# Detener Spark (opcional - se puede mantener para otros notebooks)\n",
    "# spark_loader.stop_spark()\n",
    "\n",
    "print(\"✅ Procesamiento completado\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "585bbf20",
   "metadata": {},
   "source": [
    "## 10. Mejores Prácticas y Tips\n",
    "\n",
    "### Performance:\n",
    "- Usar `cache()` para DataFrames reutilizados\n",
    "- Particionar datos por columnas que se usan frecuentemente en filtros\n",
    "- Usar formatos eficientes como Parquet\n",
    "- Evitar `collect()` en DataFrames grandes\n",
    "\n",
    "### Bases de Datos:\n",
    "- Usar particionado al leer tablas grandes\n",
    "- Filtrar en la query SQL cuando sea posible\n",
    "- Considerar índices en las tablas fuente\n",
    "- Usar connection pooling para múltiples conexiones\n",
    "\n",
    "### Desarrollo:\n",
    "- Testear con muestras pequeñas primero\n",
    "- Usar `explain()` para entender planes de ejecución\n",
    "- Monitorear Spark UI durante desarrollo\n",
    "- Configurar logs apropiadamente\n",
    "\n",
    "### Producción:\n",
    "- Configurar recursos apropiados (memoria, cores)\n",
    "- Implementar manejo de errores robusto\n",
    "- Usar checkpointing para jobs largos\n",
    "- Monitorear métricas de performance"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
