{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3b7b8046",
   "metadata": {},
   "source": [
    "# üóÑÔ∏è PySpark + MySQL - Configuraci√≥n y Uso\n",
    "\n",
    "Notebook especializado para trabajar con PySpark y MySQL sin problemas de configuraci√≥n.\n",
    "\n",
    "## üéØ Objetivos:\n",
    "- Configurar PySpark para MySQL autom√°ticamente\n",
    "- Probar conectividad\n",
    "- Cargar y procesar datos\n",
    "- Optimizar performance\n",
    "- Mejores pr√°cticas"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed6eed5a",
   "metadata": {},
   "source": [
    "## ‚öôÔ∏è 1. Configuraci√≥n Inicial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2afea53b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importar librer√≠as necesarias\n",
    "import sys\n",
    "sys.path.append('../src')\n",
    "\n",
    "from etl.mysql_spark import create_mysql_spark_connector\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.types import *\n",
    "import logging\n",
    "\n",
    "# Configurar logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "print(\"üìö Librer√≠as importadas correctamente\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfdde5ed",
   "metadata": {},
   "source": [
    "## üîó 2. Crear Conexi√≥n MySQL + Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d17594ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crear conector (ajustar config.yaml con tus credenciales)\n",
    "connector = create_mysql_spark_connector('../config.yaml')\n",
    "\n",
    "# Verificar que Spark est√© funcionando\n",
    "spark = connector.spark\n",
    "print(f\"‚úÖ Spark versi√≥n: {spark.version}\")\n",
    "print(f\"‚úÖ Aplicaci√≥n: {spark.sparkContext.appName}\")\n",
    "print(f\"‚úÖ Master: {spark.sparkContext.master}\")\n",
    "print(f\"‚úÖ Spark UI: {spark.sparkContext.uiWebUrl}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e548170",
   "metadata": {},
   "source": [
    "## üß™ 3. Probar Conectividad MySQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5a496bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Probar conexi√≥n directa a MySQL\n",
    "if connector.test_mysql_connection():\n",
    "    print(\"üéâ ¬°Conexi√≥n a MySQL exitosa!\")\n",
    "else:\n",
    "    print(\"‚ùå Problema de conexi√≥n a MySQL\")\n",
    "    print(\"\\nüîß Verifica en config.yaml:\")\n",
    "    print(\"   - host, port, database\")\n",
    "    print(\"   - user, password\")\n",
    "    print(\"   - Que el servidor MySQL est√© ejecut√°ndose\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "752bdc1c",
   "metadata": {},
   "source": [
    "## üìã 4. Explorar Base de Datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d499acd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Listar todas las tablas disponibles\n",
    "tables = connector.list_tables()\n",
    "\n",
    "print(f\"üìä Tablas encontradas ({len(tables)}):\")\n",
    "for i, table in enumerate(tables, 1):\n",
    "    print(f\"  {i}. {table}\")\n",
    "\n",
    "if not tables:\n",
    "    print(\"‚ö†Ô∏è  No se encontraron tablas o no hay permisos\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3f4f3bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Si tienes tablas, obtener informaci√≥n de una\n",
    "if tables:\n",
    "    # Cambiar por el nombre de tu tabla\n",
    "    table_name = tables[0]  # Usar la primera tabla encontrada\n",
    "    \n",
    "    print(f\"üîç Analizando tabla: {table_name}\")\n",
    "    table_info = connector.get_table_info(table_name)\n",
    "    \n",
    "    if table_info:\n",
    "        print(f\"   üìè Total filas: {table_info['total_rows']:,}\")\n",
    "        print(f\"   üìã Columnas ({len(table_info['columns'])}): {', '.join(table_info['columns'])}\")\n",
    "        print(f\"   üéØ Tipos de datos:\")\n",
    "        for col, dtype in table_info['dtypes']:\n",
    "            print(f\"      {col}: {dtype}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9821ac5",
   "metadata": {},
   "source": [
    "## üìä 5. Cargar Datos con Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e76154c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# M√©todo 1: Cargar tabla completa (para tablas peque√±as-medianas)\n",
    "if tables:\n",
    "    table_name = tables[0]  # Ajustar seg√∫n tu tabla\n",
    "    \n",
    "    # Cargar muestra primero para tablas grandes\n",
    "    print(f\"üì• Cargando muestra de {table_name}...\")\n",
    "    df_sample = connector.read_table(table_name, sample_fraction=0.1)  # 10% de muestra\n",
    "    \n",
    "    print(f\"‚úÖ Muestra cargada: {df_sample.count():,} filas\")\n",
    "    print(\"\\nüîç Primeras 5 filas:\")\n",
    "    df_sample.show(5)\n",
    "    \n",
    "    print(\"\\nüìä Schema:\")\n",
    "    df_sample.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5000ab98",
   "metadata": {},
   "outputs": [],
   "source": [
    "# M√©todo 2: Query personalizada\n",
    "if tables:\n",
    "    table_name = tables[0]\n",
    "    \n",
    "    # Ejemplo de query personalizada\n",
    "    custom_query = f\"\"\"\n",
    "    SELECT * \n",
    "    FROM {table_name} \n",
    "    LIMIT 1000\n",
    "    \"\"\"\n",
    "    \n",
    "    print(\"üìù Ejecutando query personalizada...\")\n",
    "    df_query = connector.read_query(custom_query)\n",
    "    \n",
    "    print(f\"‚úÖ Query ejecutada: {df_query.count():,} filas\")\n",
    "    df_query.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dccf85e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# M√©todo 3: Carga particionada (para tablas muy grandes)\n",
    "# Requiere una columna num√©rica para particionar\n",
    "\n",
    "# Ejemplo (ajustar seg√∫n tu tabla):\n",
    "# df_partitioned = connector.read_partitioned(\n",
    "#     table_name=\"mi_tabla\",\n",
    "#     partition_column=\"id\",  # Columna num√©rica\n",
    "#     lower_bound=1,\n",
    "#     upper_bound=100000,\n",
    "#     num_partitions=8\n",
    "# )\n",
    "\n",
    "print(\"üí° M√©todo 3 comentado - descomentar y ajustar seg√∫n tu tabla\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d9a6be2",
   "metadata": {},
   "source": [
    "## üîÑ 6. Procesamiento de Datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9c5ae73",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trabajar con el DataFrame cargado\n",
    "if 'df_sample' in locals():\n",
    "    df = df_sample  # Usar la muestra cargada\n",
    "    \n",
    "    print(\"üîÑ Realizando transformaciones...\")\n",
    "    \n",
    "    # Optimizar DataFrame\n",
    "    df = connector.optimize_dataframe(df, cache=True)\n",
    "    \n",
    "    # An√°lisis b√°sico\n",
    "    print(f\"üìä Particiones: {df.rdd.getNumPartitions()}\")\n",
    "    print(f\"üìè Total filas: {df.count():,}\")\n",
    "    \n",
    "    # Estad√≠sticas descriptivas\n",
    "    print(\"\\nüìà Estad√≠sticas:\")\n",
    "    df.describe().show()\n",
    "    \n",
    "    # Contar valores nulos\n",
    "    print(\"\\nüîç Valores nulos por columna:\")\n",
    "    null_counts = df.select([F.count(F.when(F.col(c).isNull(), c)).alias(c) for c in df.columns])\n",
    "    null_counts.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23dd0b8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ejemplo de transformaciones comunes\n",
    "if 'df' in locals():\n",
    "    print(\"üõ†Ô∏è Aplicando transformaciones...\")\n",
    "    \n",
    "    # Seleccionar columnas espec√≠ficas (ajustar seg√∫n tu tabla)\n",
    "    # df_selected = df.select(\"col1\", \"col2\", \"col3\")\n",
    "    \n",
    "    # Filtrar datos\n",
    "    # df_filtered = df.filter(F.col(\"columna\") > 100)\n",
    "    \n",
    "    # Agregar nueva columna\n",
    "    # df_with_new_col = df.withColumn(\"nueva_col\", F.lit(\"valor\"))\n",
    "    \n",
    "    # Agrupaciones\n",
    "    # df_grouped = df.groupBy(\"categoria\").agg(\n",
    "    #     F.count(\"*\").alias(\"total\"),\n",
    "    #     F.avg(\"valor\").alias(\"promedio\")\n",
    "    # )\n",
    "    \n",
    "    print(\"‚úÖ Transformaciones definidas (descomentar seg√∫n necesites)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6d26704",
   "metadata": {},
   "source": [
    "## üíæ 7. Guardar Resultados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcaeccd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Guardar en formato Parquet (recomendado)\n",
    "if 'df' in locals():\n",
    "    output_path = \"../data/processed/mysql_spark_output\"\n",
    "    \n",
    "    print(f\"üíæ Guardando datos en: {output_path}\")\n",
    "    \n",
    "    # Guardar como Parquet\n",
    "    df.coalesce(1).write.mode(\"overwrite\").parquet(output_path + \"/parquet\")\n",
    "    \n",
    "    # Guardar como CSV\n",
    "    df.coalesce(1).write.mode(\"overwrite\").option(\"header\", \"true\").csv(output_path + \"/csv\")\n",
    "    \n",
    "    print(\"‚úÖ Datos guardados exitosamente\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8accfe92",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Opcional: Escribir de vuelta a MySQL\n",
    "# Crear tabla nueva con los datos procesados\n",
    "\n",
    "# if 'df' in locals():\n",
    "#     print(\"üîÑ Escribiendo datos procesados de vuelta a MySQL...\")\n",
    "#     \n",
    "#     connector.write_table(\n",
    "#         df=df_sample.limit(100),  # Solo primeras 100 filas como ejemplo\n",
    "#         table_name=\"tabla_procesada_spark\",\n",
    "#         mode=\"overwrite\",\n",
    "#         batch_size=500\n",
    "#     )\n",
    "#     \n",
    "#     print(\"‚úÖ Datos escritos a MySQL\")\n",
    "\n",
    "print(\"üí° Escritura a MySQL comentada - descomentar si necesitas\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec2f24d5",
   "metadata": {},
   "source": [
    "## üìä 8. Monitoreo y Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57416839",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Informaci√≥n del contexto Spark\n",
    "sc = spark.sparkContext\n",
    "\n",
    "print(\"üìä INFORMACI√ìN DE SPARK\")\n",
    "print(f\"   üéØ Aplicaci√≥n ID: {sc.applicationId}\")\n",
    "print(f\"   üåê Spark UI: {sc.uiWebUrl}\")\n",
    "print(f\"   üíª Master: {sc.master}\")\n",
    "print(f\"   üîß Configuraciones activas:\")\n",
    "\n",
    "# Mostrar configuraciones importantes\n",
    "important_configs = [\n",
    "    'spark.driver.memory',\n",
    "    'spark.executor.memory', \n",
    "    'spark.sql.adaptive.enabled',\n",
    "    'spark.jars.packages'\n",
    "]\n",
    "\n",
    "for config in important_configs:\n",
    "    value = spark.conf.get(config, 'No configurado')\n",
    "    print(f\"      {config}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31356090",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verificar drivers JDBC disponibles\n",
    "print(\"üîå DRIVERS JDBC DISPONIBLES\")\n",
    "try:\n",
    "    # Verificar que el driver MySQL est√© cargado\n",
    "    test_df = spark.sql(\"SELECT 1 as test\")\n",
    "    print(\"   ‚úÖ Spark SQL funcionando\")\n",
    "    \n",
    "    # Mostrar packages cargados\n",
    "    packages = spark.conf.get('spark.jars.packages', 'No configurado')\n",
    "    print(f\"   üì¶ Packages: {packages}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"   ‚ùå Error: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fa2ee1e",
   "metadata": {},
   "source": [
    "## üßπ 9. Limpieza"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "409ed938",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Limpiar cache si fue usado\n",
    "if 'df' in locals():\n",
    "    df.unpersist()\n",
    "    print(\"üßπ Cache limpiado\")\n",
    "\n",
    "# Opcional: Cerrar Spark (descomentar si quieres)\n",
    "# connector.close()\n",
    "# print(\"üîö SparkSession cerrada\")\n",
    "\n",
    "print(\"‚úÖ Limpieza completada\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67dda5fe",
   "metadata": {},
   "source": [
    "## üõ†Ô∏è 10. Troubleshooting y Tips\n",
    "\n",
    "### ‚ùå Problemas Comunes:\n",
    "\n",
    "#### 1. **Error de conexi√≥n MySQL**\n",
    "```\n",
    "Communications link failure\n",
    "```\n",
    "**Soluci√≥n:**\n",
    "- Verificar que MySQL est√© ejecut√°ndose\n",
    "- Revisar host, port, user, password en config.yaml\n",
    "- Verificar permisos del usuario MySQL\n",
    "\n",
    "#### 2. **Driver JDBC no encontrado**\n",
    "```\n",
    "No suitable driver found\n",
    "```\n",
    "**Soluci√≥n:**\n",
    "- El conector descarga autom√°ticamente el driver\n",
    "- Verificar conexi√≥n a internet\n",
    "- Revisar spark.jars.packages en configuraci√≥n\n",
    "\n",
    "#### 3. **OutOfMemoryError**\n",
    "```\n",
    "Java heap space\n",
    "```\n",
    "**Soluci√≥n:**\n",
    "- Aumentar spark.driver.memory en config.yaml\n",
    "- Usar muestras m√°s peque√±as\n",
    "- Particionar datos grandes\n",
    "\n",
    "### ‚úÖ Mejores Pr√°cticas:\n",
    "\n",
    "1. **Performance:**\n",
    "   - Usar `cache()` para DataFrames reutilizados\n",
    "   - Particionar tablas grandes\n",
    "   - Filtrar en la query MySQL cuando sea posible\n",
    "\n",
    "2. **Memoria:**\n",
    "   - Empezar con muestras peque√±as\n",
    "   - Aumentar memoria gradualmente\n",
    "   - Usar `coalesce()` para reducir particiones\n",
    "\n",
    "3. **Desarrollo:**\n",
    "   - Probar conexiones antes de cargas grandes\n",
    "   - Usar LIMIT en queries de prueba\n",
    "   - Monitorear Spark UI durante desarrollo\n",
    "\n",
    "### üîß Configuraci√≥n Recomendada:\n",
    "\n",
    "Para tablas **peque√±as** (< 1M filas):\n",
    "```yaml\n",
    "spark:\n",
    "  driver_memory: \"2g\"\n",
    "  executor_memory: \"2g\"\n",
    "```\n",
    "\n",
    "Para tablas **medianas** (1M - 10M filas):\n",
    "```yaml\n",
    "spark:\n",
    "  driver_memory: \"4g\"\n",
    "  executor_memory: \"4g\"\n",
    "```\n",
    "\n",
    "Para tablas **grandes** (> 10M filas):\n",
    "```yaml\n",
    "spark:\n",
    "  driver_memory: \"8g\"\n",
    "  executor_memory: \"8g\"\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
