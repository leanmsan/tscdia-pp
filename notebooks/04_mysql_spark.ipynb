{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3b7b8046",
   "metadata": {},
   "source": [
    "# 🗄️ PySpark + MySQL - Configuración y Uso\n",
    "\n",
    "Notebook especializado para trabajar con PySpark y MySQL sin problemas de configuración.\n",
    "\n",
    "## 🎯 Objetivos:\n",
    "- Configurar PySpark para MySQL automáticamente\n",
    "- Probar conectividad\n",
    "- Cargar y procesar datos\n",
    "- Optimizar performance\n",
    "- Mejores prácticas"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed6eed5a",
   "metadata": {},
   "source": [
    "## ⚙️ 1. Configuración Inicial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2afea53b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importar librerías necesarias\n",
    "import sys\n",
    "sys.path.append('../src')\n",
    "\n",
    "from etl.mysql_spark import create_mysql_spark_connector\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.types import *\n",
    "import logging\n",
    "\n",
    "# Configurar logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "print(\"📚 Librerías importadas correctamente\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfdde5ed",
   "metadata": {},
   "source": [
    "## 🔗 2. Crear Conexión MySQL + Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d17594ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crear conector (ajustar config.yaml con tus credenciales)\n",
    "connector = create_mysql_spark_connector('../config.yaml')\n",
    "\n",
    "# Verificar que Spark esté funcionando\n",
    "spark = connector.spark\n",
    "print(f\"✅ Spark versión: {spark.version}\")\n",
    "print(f\"✅ Aplicación: {spark.sparkContext.appName}\")\n",
    "print(f\"✅ Master: {spark.sparkContext.master}\")\n",
    "print(f\"✅ Spark UI: {spark.sparkContext.uiWebUrl}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e548170",
   "metadata": {},
   "source": [
    "## 🧪 3. Probar Conectividad MySQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5a496bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Probar conexión directa a MySQL\n",
    "if connector.test_mysql_connection():\n",
    "    print(\"🎉 ¡Conexión a MySQL exitosa!\")\n",
    "else:\n",
    "    print(\"❌ Problema de conexión a MySQL\")\n",
    "    print(\"\\n🔧 Verifica en config.yaml:\")\n",
    "    print(\"   - host, port, database\")\n",
    "    print(\"   - user, password\")\n",
    "    print(\"   - Que el servidor MySQL esté ejecutándose\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "752bdc1c",
   "metadata": {},
   "source": [
    "## 📋 4. Explorar Base de Datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d499acd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Listar todas las tablas disponibles\n",
    "tables = connector.list_tables()\n",
    "\n",
    "print(f\"📊 Tablas encontradas ({len(tables)}):\")\n",
    "for i, table in enumerate(tables, 1):\n",
    "    print(f\"  {i}. {table}\")\n",
    "\n",
    "if not tables:\n",
    "    print(\"⚠️  No se encontraron tablas o no hay permisos\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3f4f3bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Si tienes tablas, obtener información de una\n",
    "if tables:\n",
    "    # Cambiar por el nombre de tu tabla\n",
    "    table_name = tables[0]  # Usar la primera tabla encontrada\n",
    "    \n",
    "    print(f\"🔍 Analizando tabla: {table_name}\")\n",
    "    table_info = connector.get_table_info(table_name)\n",
    "    \n",
    "    if table_info:\n",
    "        print(f\"   📏 Total filas: {table_info['total_rows']:,}\")\n",
    "        print(f\"   📋 Columnas ({len(table_info['columns'])}): {', '.join(table_info['columns'])}\")\n",
    "        print(f\"   🎯 Tipos de datos:\")\n",
    "        for col, dtype in table_info['dtypes']:\n",
    "            print(f\"      {col}: {dtype}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9821ac5",
   "metadata": {},
   "source": [
    "## 📊 5. Cargar Datos con Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e76154c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Método 1: Cargar tabla completa (para tablas pequeñas-medianas)\n",
    "if tables:\n",
    "    table_name = tables[0]  # Ajustar según tu tabla\n",
    "    \n",
    "    # Cargar muestra primero para tablas grandes\n",
    "    print(f\"📥 Cargando muestra de {table_name}...\")\n",
    "    df_sample = connector.read_table(table_name, sample_fraction=0.1)  # 10% de muestra\n",
    "    \n",
    "    print(f\"✅ Muestra cargada: {df_sample.count():,} filas\")\n",
    "    print(\"\\n🔍 Primeras 5 filas:\")\n",
    "    df_sample.show(5)\n",
    "    \n",
    "    print(\"\\n📊 Schema:\")\n",
    "    df_sample.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5000ab98",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Método 2: Query personalizada\n",
    "if tables:\n",
    "    table_name = tables[0]\n",
    "    \n",
    "    # Ejemplo de query personalizada\n",
    "    custom_query = f\"\"\"\n",
    "    SELECT * \n",
    "    FROM {table_name} \n",
    "    LIMIT 1000\n",
    "    \"\"\"\n",
    "    \n",
    "    print(\"📝 Ejecutando query personalizada...\")\n",
    "    df_query = connector.read_query(custom_query)\n",
    "    \n",
    "    print(f\"✅ Query ejecutada: {df_query.count():,} filas\")\n",
    "    df_query.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dccf85e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Método 3: Carga particionada (para tablas muy grandes)\n",
    "# Requiere una columna numérica para particionar\n",
    "\n",
    "# Ejemplo (ajustar según tu tabla):\n",
    "# df_partitioned = connector.read_partitioned(\n",
    "#     table_name=\"mi_tabla\",\n",
    "#     partition_column=\"id\",  # Columna numérica\n",
    "#     lower_bound=1,\n",
    "#     upper_bound=100000,\n",
    "#     num_partitions=8\n",
    "# )\n",
    "\n",
    "print(\"💡 Método 3 comentado - descomentar y ajustar según tu tabla\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d9a6be2",
   "metadata": {},
   "source": [
    "## 🔄 6. Procesamiento de Datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9c5ae73",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trabajar con el DataFrame cargado\n",
    "if 'df_sample' in locals():\n",
    "    df = df_sample  # Usar la muestra cargada\n",
    "    \n",
    "    print(\"🔄 Realizando transformaciones...\")\n",
    "    \n",
    "    # Optimizar DataFrame\n",
    "    df = connector.optimize_dataframe(df, cache=True)\n",
    "    \n",
    "    # Análisis básico\n",
    "    print(f\"📊 Particiones: {df.rdd.getNumPartitions()}\")\n",
    "    print(f\"📏 Total filas: {df.count():,}\")\n",
    "    \n",
    "    # Estadísticas descriptivas\n",
    "    print(\"\\n📈 Estadísticas:\")\n",
    "    df.describe().show()\n",
    "    \n",
    "    # Contar valores nulos\n",
    "    print(\"\\n🔍 Valores nulos por columna:\")\n",
    "    null_counts = df.select([F.count(F.when(F.col(c).isNull(), c)).alias(c) for c in df.columns])\n",
    "    null_counts.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23dd0b8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ejemplo de transformaciones comunes\n",
    "if 'df' in locals():\n",
    "    print(\"🛠️ Aplicando transformaciones...\")\n",
    "    \n",
    "    # Seleccionar columnas específicas (ajustar según tu tabla)\n",
    "    # df_selected = df.select(\"col1\", \"col2\", \"col3\")\n",
    "    \n",
    "    # Filtrar datos\n",
    "    # df_filtered = df.filter(F.col(\"columna\") > 100)\n",
    "    \n",
    "    # Agregar nueva columna\n",
    "    # df_with_new_col = df.withColumn(\"nueva_col\", F.lit(\"valor\"))\n",
    "    \n",
    "    # Agrupaciones\n",
    "    # df_grouped = df.groupBy(\"categoria\").agg(\n",
    "    #     F.count(\"*\").alias(\"total\"),\n",
    "    #     F.avg(\"valor\").alias(\"promedio\")\n",
    "    # )\n",
    "    \n",
    "    print(\"✅ Transformaciones definidas (descomentar según necesites)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6d26704",
   "metadata": {},
   "source": [
    "## 💾 7. Guardar Resultados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcaeccd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Guardar en formato Parquet (recomendado)\n",
    "if 'df' in locals():\n",
    "    output_path = \"../data/processed/mysql_spark_output\"\n",
    "    \n",
    "    print(f\"💾 Guardando datos en: {output_path}\")\n",
    "    \n",
    "    # Guardar como Parquet\n",
    "    df.coalesce(1).write.mode(\"overwrite\").parquet(output_path + \"/parquet\")\n",
    "    \n",
    "    # Guardar como CSV\n",
    "    df.coalesce(1).write.mode(\"overwrite\").option(\"header\", \"true\").csv(output_path + \"/csv\")\n",
    "    \n",
    "    print(\"✅ Datos guardados exitosamente\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8accfe92",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Opcional: Escribir de vuelta a MySQL\n",
    "# Crear tabla nueva con los datos procesados\n",
    "\n",
    "# if 'df' in locals():\n",
    "#     print(\"🔄 Escribiendo datos procesados de vuelta a MySQL...\")\n",
    "#     \n",
    "#     connector.write_table(\n",
    "#         df=df_sample.limit(100),  # Solo primeras 100 filas como ejemplo\n",
    "#         table_name=\"tabla_procesada_spark\",\n",
    "#         mode=\"overwrite\",\n",
    "#         batch_size=500\n",
    "#     )\n",
    "#     \n",
    "#     print(\"✅ Datos escritos a MySQL\")\n",
    "\n",
    "print(\"💡 Escritura a MySQL comentada - descomentar si necesitas\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec2f24d5",
   "metadata": {},
   "source": [
    "## 📊 8. Monitoreo y Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57416839",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Información del contexto Spark\n",
    "sc = spark.sparkContext\n",
    "\n",
    "print(\"📊 INFORMACIÓN DE SPARK\")\n",
    "print(f\"   🎯 Aplicación ID: {sc.applicationId}\")\n",
    "print(f\"   🌐 Spark UI: {sc.uiWebUrl}\")\n",
    "print(f\"   💻 Master: {sc.master}\")\n",
    "print(f\"   🔧 Configuraciones activas:\")\n",
    "\n",
    "# Mostrar configuraciones importantes\n",
    "important_configs = [\n",
    "    'spark.driver.memory',\n",
    "    'spark.executor.memory', \n",
    "    'spark.sql.adaptive.enabled',\n",
    "    'spark.jars.packages'\n",
    "]\n",
    "\n",
    "for config in important_configs:\n",
    "    value = spark.conf.get(config, 'No configurado')\n",
    "    print(f\"      {config}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31356090",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verificar drivers JDBC disponibles\n",
    "print(\"🔌 DRIVERS JDBC DISPONIBLES\")\n",
    "try:\n",
    "    # Verificar que el driver MySQL esté cargado\n",
    "    test_df = spark.sql(\"SELECT 1 as test\")\n",
    "    print(\"   ✅ Spark SQL funcionando\")\n",
    "    \n",
    "    # Mostrar packages cargados\n",
    "    packages = spark.conf.get('spark.jars.packages', 'No configurado')\n",
    "    print(f\"   📦 Packages: {packages}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"   ❌ Error: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fa2ee1e",
   "metadata": {},
   "source": [
    "## 🧹 9. Limpieza"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "409ed938",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Limpiar cache si fue usado\n",
    "if 'df' in locals():\n",
    "    df.unpersist()\n",
    "    print(\"🧹 Cache limpiado\")\n",
    "\n",
    "# Opcional: Cerrar Spark (descomentar si quieres)\n",
    "# connector.close()\n",
    "# print(\"🔚 SparkSession cerrada\")\n",
    "\n",
    "print(\"✅ Limpieza completada\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67dda5fe",
   "metadata": {},
   "source": [
    "## 🛠️ 10. Troubleshooting y Tips\n",
    "\n",
    "### ❌ Problemas Comunes:\n",
    "\n",
    "#### 1. **Error de conexión MySQL**\n",
    "```\n",
    "Communications link failure\n",
    "```\n",
    "**Solución:**\n",
    "- Verificar que MySQL esté ejecutándose\n",
    "- Revisar host, port, user, password en config.yaml\n",
    "- Verificar permisos del usuario MySQL\n",
    "\n",
    "#### 2. **Driver JDBC no encontrado**\n",
    "```\n",
    "No suitable driver found\n",
    "```\n",
    "**Solución:**\n",
    "- El conector descarga automáticamente el driver\n",
    "- Verificar conexión a internet\n",
    "- Revisar spark.jars.packages en configuración\n",
    "\n",
    "#### 3. **OutOfMemoryError**\n",
    "```\n",
    "Java heap space\n",
    "```\n",
    "**Solución:**\n",
    "- Aumentar spark.driver.memory en config.yaml\n",
    "- Usar muestras más pequeñas\n",
    "- Particionar datos grandes\n",
    "\n",
    "### ✅ Mejores Prácticas:\n",
    "\n",
    "1. **Performance:**\n",
    "   - Usar `cache()` para DataFrames reutilizados\n",
    "   - Particionar tablas grandes\n",
    "   - Filtrar en la query MySQL cuando sea posible\n",
    "\n",
    "2. **Memoria:**\n",
    "   - Empezar con muestras pequeñas\n",
    "   - Aumentar memoria gradualmente\n",
    "   - Usar `coalesce()` para reducir particiones\n",
    "\n",
    "3. **Desarrollo:**\n",
    "   - Probar conexiones antes de cargas grandes\n",
    "   - Usar LIMIT en queries de prueba\n",
    "   - Monitorear Spark UI durante desarrollo\n",
    "\n",
    "### 🔧 Configuración Recomendada:\n",
    "\n",
    "Para tablas **pequeñas** (< 1M filas):\n",
    "```yaml\n",
    "spark:\n",
    "  driver_memory: \"2g\"\n",
    "  executor_memory: \"2g\"\n",
    "```\n",
    "\n",
    "Para tablas **medianas** (1M - 10M filas):\n",
    "```yaml\n",
    "spark:\n",
    "  driver_memory: \"4g\"\n",
    "  executor_memory: \"4g\"\n",
    "```\n",
    "\n",
    "Para tablas **grandes** (> 10M filas):\n",
    "```yaml\n",
    "spark:\n",
    "  driver_memory: \"8g\"\n",
    "  executor_memory: \"8g\"\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
