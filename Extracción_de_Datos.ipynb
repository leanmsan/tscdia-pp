{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Instalamos los paquetes necesarios para trabajar con Spark en Google Colab.\n",
        "\n",
        "- `pyspark`: permite utilizar Apache Spark desde Python.\n",
        "\n",
        "- `findspark`: facilita la integración de Spark con notebooks al detectar automáticamente la instalación de Spark.\n",
        "\n",
        "- La opción `-q` silencia la salida para que el proceso sea más limpio."
      ],
      "metadata": {
        "id": "rx4O2-yTQDKV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pyspark findspark -q"
      ],
      "metadata": {
        "id": "MJaTLcvbkyJX"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Descargamos el archivo `.jar` del conector `JDBC` de `MySQL`, necesario para que `PySpark` pueda conectarse a bases de datos `MySQL`.\n",
        "El archivo contiene las clases que permiten establecer la conexión entre Spark y `MySQL` usando `JDBC`.\n",
        "La opción `-q` se usa para suprimir la salida del proceso de descarga."
      ],
      "metadata": {
        "id": "e7UmatsdQPNO"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "640e01ff"
      },
      "source": [
        "!wget https://repo.maven.apache.org/maven2/mysql/mysql-connector-java/8.0.30/mysql-connector-java-8.0.30.jar -q"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Inicializamos `PySpark` dentro del entorno de ejecución de Colab.\n",
        "La librería `findspark` configura las variables de entorno necesarias para que `Python` pueda encontrar e importar los módulos de Spark correctamente."
      ],
      "metadata": {
        "id": "YW22C8c9QihK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import findspark\n",
        "\n",
        "findspark.init()"
      ],
      "metadata": {
        "id": "WTguTz-lk7f9"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Importamos los módulos necesarios para trabajar con PySpark y leer datos:\n",
        "\n",
        "- `SparkSession` nos permite iniciar una sesión de Spark.\n",
        "\n",
        "- `col` se usa para manipular columnas en transformaciones de `DataFrames`.\n",
        "\n",
        "- `json` nos permite cargar las credenciales almacenadas en un archivo `.json`."
      ],
      "metadata": {
        "id": "gaTxQRDIQuG3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import col\n",
        "import json"
      ],
      "metadata": {
        "id": "KRhtBM_qk6E9"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Creamos una sesión de `Spark` y le indicamos la ruta al conector `JDBC` de `MySQL` mediante `.config(\"spark.jars\", ...)`. Esto permite establecer conexiones entre `PySpark` y una base de datos `MySQL`."
      ],
      "metadata": {
        "id": "yJi5SIccQ6ho"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cf3f2fb6",
        "outputId": "d45f5a93-8706-4659-f816-cad79c56e87d"
      },
      "source": [
        "spark = SparkSession.builder \\\n",
        "    .appName('PySpark MySQL Connection') \\\n",
        "    .config('spark.jars', 'mysql-connector-java-8.0.30.jar') \\\n",
        "    .getOrCreate()\n",
        "\n",
        "print('Spark configurado con el conector de MySQL.')"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Spark configurado con el conector de MySQL.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DlwuYj5EFP2C",
        "outputId": "1bd96998-c958-4411-e8f7-437e537b9260"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Definimos la ruta al archivo `credentials.json`, que contiene las credenciales necesarias (como usuario, contraseña, host y base de datos) para conectarnos de forma segura a `MySQL` desde `PySpark`."
      ],
      "metadata": {
        "id": "Aa0_3jz3RIl9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "credentials_file = '/content/drive/MyDrive/TSCDeIA - Práctica Profesionalizante/Procesamiento de Datos/credentials.json'"
      ],
      "metadata": {
        "id": "CTb4VtYjE-fb"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Abrimos el archivo `credentials.json` y cargamos su contenido como un diccionario usando `json.load`. Luego, extraemos los datos de conexión a la base de datos (host, puerto, usuario, contraseña, nombre de base y tabla) utilizando el método `.get()` para evitar errores si alguna clave no existe. Esto nos permite mantener las credenciales fuera del código fuente y trabajar de forma más segura."
      ],
      "metadata": {
        "id": "bwUGmBr6RVIN"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "80d60ea2",
        "outputId": "fda155d4-a254-4b21-b9ea-90e2f8987338"
      },
      "source": [
        "with open(credentials_file, 'r') as f:\n",
        "  credentials = json.load(f)\n",
        "\n",
        "db_host = credentials.get('db_host')\n",
        "db_port = credentials.get('db_port')\n",
        "db_user = credentials.get('db_user')\n",
        "db_password = credentials.get('db_password')\n",
        "db_name = credentials.get('db_name')\n",
        "db_table = credentials.get('db_table')\n",
        "\n",
        "print('Credenciales cargadas desde el archivo credentials.json.')"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Credenciales cargadas desde el archivo credentials.json.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "A partir de las credenciales cargadas, construimos la `URL` de conexión `JDBC` que `Spark` usará para acceder a la base de datos `MySQL`. Esta `URL` incluye el host, puerto y nombre de la base, además de un parámetro adicional (zeroDateTimeBehavior=round) que permite manejar valores de fecha nula en `MySQL` sin generar errores."
      ],
      "metadata": {
        "id": "0rN5l2nqRhGM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "jdbc_url = f\"jdbc:mysql://{db_host}:{db_port}/{db_name}?zeroDateTimeBehavior=round\"\n",
        "\n",
        "print('URL contruida a partir de las credenciales corretamente')"
      ],
      "metadata": {
        "id": "T1Z5cdAXHG_U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Este bloque de código verifica que todas las variables necesarias estén definidas y que `db_table` tenga un valor válido. Si es así, intenta conectarse a la base de datos `MySQL` mediante el conector `JDBC` y cargar la tabla especificada dentro de un `DataFrame` de `Spark`. Esta operación nos permite integrar datos relacionales con el ecosistema distribuido de `PySpark`, facilitando su procesamiento posterior. Además, se maneja cualquier error que pueda surgir durante la conexión o la lectura, mostrando un mensaje claro para depuración."
      ],
      "metadata": {
        "id": "lZAdvqLJRyvE"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "35aa34e9"
      },
      "source": [
        "if 'jdbc_url' in locals() and 'db_user' in locals() and 'db_password' in locals() and 'db_table' in locals() and db_table is not None:\n",
        "    try:\n",
        "        print(f\"Intentando leer la tabla '{db_table}' de la base de datos MySQL...\")\n",
        "        data = spark.read.format('jdbc') \\\n",
        "            .option('url', jdbc_url) \\\n",
        "            .option('driver', 'com.mysql.cj.jdbc.Driver') \\\n",
        "            .option('dbtable', db_table) \\\n",
        "            .option('user', db_user) \\\n",
        "            .option('password', db_password) \\\n",
        "            .load()\n",
        "\n",
        "        print('Datos leídos exitosamente en un DataFrame de Spark.')\n",
        "    except Exception as e:\n",
        "        print(f'Ocurrió un error al intentar leer la tabla: {e}')\n",
        "else:\n",
        "    print(\"Error: Las credenciales o el nombre de la tabla no están disponibles o 'db_table' es None.\")\n",
        "    print(\"Por favor, asegúrate de ejecutar la celda anterior para cargar las credenciales y verificar que 'db_table' esté presente y tenga un valor en credentials.json.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Este bloque nos permite visualizar la estructura del `DataFrame` cargado desde la base de datos. Al utilizar `printSchema()`, `PySpark` muestra los nombres de las columnas, sus tipos de datos y si permiten valores nulos. Esta información es clave para validar que los datos se hayan leído correctamente y que la inferencia de tipos sea coherente con el esquema de la base original. Nos facilita además planificar los pasos posteriores de transformación o limpieza."
      ],
      "metadata": {
        "id": "AW3tzqJqSMzL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(f'Esquema del DataFrame:')\n",
        "data.printSchema()"
      ],
      "metadata": {
        "id": "uSPEdnqxFT9V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Utilizamos la función `show()` de un `DataFrame` de `Spark` para visualizar las primeras filas de los datos cargados. Primero, imprimimos un mensaje informativo para indicar que se mostrarán las primeras filas. Luego, con `data.show()`, se despliegan por defecto las primeras 20 filas del `DataFrame` en formato tabular, lo que permite una inspección rápida del contenido y la estructura de los datos."
      ],
      "metadata": {
        "id": "f4nkZK5kSgAd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print('Mostrando las primeras filas del DataFrame:')\n",
        "data.show()"
      ],
      "metadata": {
        "id": "wvk6zSw9FTBk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Obtenemos una lista con los nombres de todas las columnas del `DataFrame` original `data`. La propiedad `.columns` devuelve una lista de strings que representa cada nombre de columna, lo cual es útil para conocer la estructura del `DataFrame` y poder manipular o analizar las columnas de manera programática."
      ],
      "metadata": {
        "id": "UaY74BUqS74P"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "columns = data.columns"
      ],
      "metadata": {
        "id": "Qctdn61rGBN6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Definimos una lista llamada `excluded_columns` que contiene los nombres de las columnas que queremos excluir del análisis o procesamiento posterior. Esta lista incluye campos relacionados con datos personales, metadatos de auditoría y observaciones, que pueden no ser relevantes o necesarias para ciertas operaciones de limpieza, modelado o visualización."
      ],
      "metadata": {
        "id": "YCzwkGxRTS5h"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7a1b46fa"
      },
      "source": [
        "excluded_columns = [\"archivo\", \"apellido\", \"nombre\", \"dni\", \"updated_at\", \"deleted_at\", \"created_by\", \"updated_by\", \"deleted_by\", \"observaciones\", \"resultado_laboratorio\"]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Creamos una nueva lista llamada `selected_columns` que contiene todas las columnas originales excepto las que están en la lista `excluded_columns`. Para esto, usamos una comprensión de listas que filtra las columnas que no están en la lista de exclusión. Luego, generamos un nuevo `DataFrame` data seleccionando únicamente las columnas de `selected_columns` mediante el método `.select()`, lo que permite trabajar con un subconjunto de columnas relevante para nuestro análisis o procesamiento."
      ],
      "metadata": {
        "id": "UG2NScFvTvnl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "selected_columns = [col for col in columns if col not in excluded_columns]\n",
        "\n",
        "data = data.select(*selected_columns)"
      ],
      "metadata": {
        "id": "onzhKqmkGJ-m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Imprimimos en consola el esquema del nuevo `DataFrame` data para verificar la estructura y tipos de datos de las columnas seleccionadas. Primero mostramos un mensaje informativo y luego usamos el método `printSchema()` de `Spark`, que despliega una representación jerárquica con los nombres de las columnas y sus tipos de datos, facilitando la inspección rápida del `DataFrame`."
      ],
      "metadata": {
        "id": "HY5QNTy6UAV6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Esquema del DataFrame con columnas seleccionadas:\")\n",
        "data.printSchema()"
      ],
      "metadata": {
        "id": "NLNtvtzbGJ7L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Mostramos en consola un mensaje informativo y luego utilizamos el método `show()` del `DataFrame` data para visualizar las primeras filas con las columnas seleccionadas. Esto permite verificar rápidamente el contenido y confirmar que la selección de columnas se realizó correctamente."
      ],
      "metadata": {
        "id": "h60gQwtJUaaF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print('Mostrando las primeras filas del DataFrame con columnas seleccionadas:')\n",
        "data.show()"
      ],
      "metadata": {
        "id": "sKgSu5lrGUbR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Definimos la columna `establecimiento_notificador` como la que utilizaremos para aplicar un filtro en el DataFrame. Luego construimos una condición de filtrado que verifica que los valores de esa columna no sean nulos `(isNotNull())` y que además no sean cadenas vacías (`!= \"\"`). Esta condición permite limpiar los datos para trabajar solo con registros que tengan un valor válido en la columna especificada."
      ],
      "metadata": {
        "id": "uK0C4D5YUjvn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "filter_column = \"establecimiento_notificador\"\n",
        "\n",
        "filter_selected = col(filter_column).isNotNull() & (col(filter_column) != \"\")"
      ],
      "metadata": {
        "id": "xIYKbbmLJX4k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Aplicamos un filtro al `DataFrame` data utilizando la condición almacenada en `filter_selected`, generando un nuevo `DataFrame` llamado `data_v2`, que contiene solo las filas que cumplen con dicho filtro. Luego, imprimimos en consola la cantidad de registros antes y después del filtrado, para así poder comparar y verificar cuántos registros fueron retenidos tras aplicar la condición de filtro."
      ],
      "metadata": {
        "id": "TE_FVsW2V9LC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data_v2 = data.filter(filter_selected)\n",
        "\n",
        "print(f\"Número de registros antes del filtrado por '{filter_column}': {data.count()}\")\n",
        "print(f\"Número de registros después del filtrado por '{filter_column}': {data_v2.count()}\")"
      ],
      "metadata": {
        "id": "1o_gesUGJbEd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Mostramos las primeras filas del `DataFrame` filtrado según la columna especificada en `filter_column`. A continuación, con el método `.show()` mostramos esas filas para verificar visualmente el resultado del filtrado aplicado."
      ],
      "metadata": {
        "id": "-dWmYnyFWkoJ"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7c0d83ea"
      },
      "source": [
        "print(f\"Mostrando las primeras filas del DataFrame filtrado por '{filter_column}':\")\n",
        "data_v2.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Convertimos el `DataFrame` de `Spark` `data_v2` a un `DataFrame` de `pandas` utilizando el método `toPandas()`. Esta conversión permite trabajar con las herramientas y funciones de `pandas`, que son muy usadas para análisis y visualización en memoria. Si la conversión es exitosa, imprimimos un mensaje de confirmación, mostramos las primeras filas con `display()` y también la cantidad total de registros en el `DataFrame` de `pandas`. En caso de que ocurra algún error durante la conversión, lo capturamos e imprimimos un mensaje con la descripción del problema."
      ],
      "metadata": {
        "id": "M7nr6jgiW2rU"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "66153020"
      },
      "source": [
        "# Convertir el DataFrame de Spark a un DataFrame de pandas\n",
        "try:\n",
        "    data_pd = data_v2.toPandas()\n",
        "    print('DataFrame de Spark convertido exitosamente a DataFrame de pandas.')\n",
        "    print('Mostrando las primeras filas del DataFrame de pandas:')\n",
        "    display(data_pd.head())\n",
        "    print(f'Número de registros en el DataFrame de pandas: {len(data_pd)}')\n",
        "except Exception as e:\n",
        "    print(f'Ocurrió un error al convertir el DataFrame a pandas: {e}')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Exportamos el `DataFrame` de pandas `data_pd` a un archivo `CSV` en la ruta especificada dentro de Google Drive. El parámetro `index=False` indica que no se incluirá la columna de índices en el archivo exportado, manteniendo solo los datos originales en el `CSV`."
      ],
      "metadata": {
        "id": "E9enlbkxXd-M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data_pd.to_csv('/content/drive/MyDrive/TSCDeIA - Práctica Profesionalizante/Procesamiento de Datos/data.csv', index=False)"
      ],
      "metadata": {
        "id": "1moqWU7WNXU_"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}